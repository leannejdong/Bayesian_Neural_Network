\documentclass[11pt, oneside, reqno]{amsart}
\usepackage{fullpage}
\usepackage{amssymb,latexsym,amsmath,verbatim,layout,hyperref,amsthm,color}  
\numberwithin{equation}{section}
\usepackage[T1]{fontenc}
\usepackage[light,math]{anttor}
\usepackage {mathrsfs}
%\DeclareGraphicsExtensions{.pdf,.png,.jpg}                 
\DeclareMathOperator*{\esssup}{ess\,sup}     
% AMS Theorems  
\theoremstyle{plain}% default
%\newtheorem{name}[counter]{text}[section]
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{q}{Question}
\newtheorem*{ans}{Answer}
\newtheorem*{cor}{Corollary}


\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{exmp}[thm]{Example}


\theoremstyle{remark}
\newtheorem*{rmk}{Remark}
\newtheorem*{note}{Note}
\newtheorem{case}{Case}
\newtheorem{ex}{Exercise}
\newtheorem{eg}{Example}
\usepackage{hyperref}
\def\ci{\perp\!\!\!\perp}
\newcommand*{\bigchi}{\mbox{\Large$\chi$}} 
\renewcommand{\vec}[1]{\mathbf{#1}}


\newcommand{\e}{\mathbf{e}} 
    

\newcommand{\vgrad}{\mathbf{\nabla}}     
\newcommand{\vlap}{\mathbf{\Delta}}     
\newcommand{\sph}{\mathbb{S}}
%	x=  x,\quad  u,\quad \vgrad,\quad \vlap,\quad \sph
\newcommand{\R}{\mathbb{R}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}
%\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\mb}[1]{\mathbf {#1}}
\newcommand{\D}[2]{\frac{\partial #1}{\partial #2}} 
\newcommand{\DD}[2]{\frac{\partial^2 #1}{\partial #2^2}} 
%\renewcommand{\vec}[1]{\mathbf{#1}}%
\newcommand{\pa}[1]{\partial {#1_{\sub{#1}}}}    
\newcommand\relphantom[1]{\mathrel{\plantom{#1}}}    
\title{Neural Network: Assignments}
\author{Leanne Dong
}
\date{\today}
\begin{document}
	\maketitle
\section{Assignment 1: Backpropagation}
\subsection{4bit dataset with an additional layer}
I designed my code to accommodate one extra layer. The particular parameter choices follow Rohit's code. I suspect I will get a better performance with a higher learn rate and a lower MSE tolderance rate.
\\

With topology $[4,4,1]$ over 10 trials:

total training $+$ eval time: $20.59$ sec

Performance: MSE (test) $= 0.014$, accuracy $= 98.75\%$
\\

With  topoloogy $[4,4,4,1]$ over 10 trials:

total training $+$ eval time: $31.63$ sec

Performance: MSE (test) $= 0.0017$, accuracy $= 99.375\%$

\subsection{Iris dataset with an additional layer}

I have chosen to add only one layer.

With topology $[4,4,2]$ over 10 trials:

total training $+$ eval time: $30.56$ sec

Performance: MSE (test) $= 0.03$, accuracy $= 95.75\%$
\\

With  topoloogy $[4,4,4,2]$ over 10 trials:

total training $+$ eval time: $31.61$ sec

Performance: MSE (test) $= 0.0093$, accuracy $= 94.75\%$

The results do not seem dramatically different by adding more layers.

\section{Assignment 2: SGD vs MCMC}
Here we will compare the performance of MCMC-SGD with one and two hidden layers on the Sunspot data set.

Then we will compare the performance of MCC FNN subjects to variation of step size and sample size.

\section{SDG with one hidden layer}
I follow the parameters Rohit has chosen. With 10 test instances, the test MSE is about $0.028$ with Stdv $0.0018$.

\section{SDG with one hidden layer}
There is no significant different found with added layer.

\begin{comment}
\begin{table}[ht]
\begin{center}
\begin{tabular}{ |c|c|c|} 
\hline
 & train & test  \\ 
 \hline\hline
MSE & 0.0011 & 0.0012  \\ 
Epochs & 1700 & 1800  \\ 
 Time & 19 & 20\\
 \hline
\end{tabular}
\end{center}
\end{table}

\begin{table}[ht]
\begin{center}
\begin{tabular}{ |c|c|c|} 
\hline
 & mean & sdtc  \\ 
 \hline\hline
trainPerf & 93.125 & 12.327  \\ 
testPerf & 98.75 & 2.5  \\ 
 Time & 1.95 & 0.53\\
 \hline
\end{tabular}
\end{center}
\end{table}  
\end{comment}
\end{document}