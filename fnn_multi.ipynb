{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For input: [0, 0]  Output --> [0.9862388782585219] \tTarget:  [0]\n",
      "For input: [0, 1]  Output --> [0.9403654749511677] \tTarget:  [1]\n",
      "For input: [1, 0]  Output --> [0.9413723342408598] \tTarget:  [1]\n",
      "For input: [1, 1]  Output --> [0.8843374629813917] \tTarget:  [1]\n"
     ]
    }
   ],
   "source": [
    "#Backpropagation algorithm written in Python by annanay25.\n",
    "import string\n",
    "import math\n",
    "import random\n",
    "\n",
    "class Neural:\n",
    "\tdef __init__(self, pattern):\n",
    "\t\t#\n",
    "\t\t# Lets take 2 input nodes, 3 hidden nodes and 1 output node.\n",
    "\t\t# Hence, Number of nodes in input(ni)=2, hidden(nh)=3, output(no)=1.\n",
    "\t\t#\n",
    "\t\tself.ni=3\n",
    "\t\tself.nh=3\n",
    "\t\tself.no=1\n",
    "\n",
    "\t\t#\n",
    "\t\t# Now we need node weights. We'll make a two dimensional array that maps node from one layer to the next.\n",
    "\t\t# i-th node of one layer to j-th node of the next.\n",
    "\t\t#\n",
    "\t\tself.wih = []\n",
    "\t\tfor i in range(self.ni):\n",
    "\t\t\tself.wih.append([0.0]*self.nh)\n",
    "\n",
    "\t\tself.who = []\n",
    "\t\tfor j in range(self.nh):\n",
    "\t\t\tself.who.append([0.0]*self.no)\n",
    "\n",
    "\t\t#\n",
    "\t\t# Now that weight matrices are created, make the activation matrices.\n",
    "\t\t#\n",
    "\t\tself.ai, self.ah, self.ao = [],[],[]\n",
    "\t\tself.ai=[1.0]*self.ni\n",
    "\t\tself.ah=[1.0]*self.nh\n",
    "\t\tself.ao=[1.0]*self.no\n",
    "\n",
    "\t\t#\n",
    "\t\t# To ensure node weights are randomly assigned, with some bounds on values, we pass it through randomizeMatrix()\n",
    "\t\t#\n",
    "\t\trandomizeMatrix(self.wih,-0.2,0.2)\n",
    "\t\trandomizeMatrix(self.who,-2.0,2.0)\n",
    "\n",
    "\t\t#\n",
    "\t\t# To incorporate momentum factor, introduce another array for the 'previous change'.\n",
    "\t\t#\n",
    "\t\tself.cih = []\n",
    "\t\tself.cho = []\n",
    "\t\tfor i in range(self.ni):\n",
    "\t\t\tself.cih.append([0.0]*self.nh)\n",
    "\t\tfor j in range(self.nh):\n",
    "\t\t\tself.cho.append([0.0]*self.no)\n",
    "\n",
    "\t# backpropagate() takes as input, the patterns entered, the target values and the obtained values.\n",
    "\t# Based on these values, it adjusts the weights so as to balance out the error.\n",
    "\t# Also, now we have M, N for momentum and learning factors respectively.\n",
    "\tdef backpropagate(self, inputs, expected, output, N=0.5, M=0.1):\n",
    "\t\t# We introduce a new matrix called the deltas (error) for the two layers output and hidden layer respectively.\n",
    "\t\toutput_deltas = [0.0]*self.no\n",
    "\t\tfor k in range(self.no):\n",
    "\t\t\t# Error is equal to (Target value - Output value)\n",
    "\t\t\terror = expected[k] - output[k]\n",
    "\t\t\toutput_deltas[k]=error*dsigmoid(self.ao[k])\n",
    "\n",
    "\t\t# Change weights of hidden to output layer accordingly.\n",
    "\t\tfor j in range(self.nh):\n",
    "\t\t\tfor k in range(self.no):\n",
    "\t\t\t\tdelta_weight = self.ah[j] * output_deltas[k]\n",
    "\t\t\t\tself.who[j][k]+= M*self.cho[j][k] + N*delta_weight\n",
    "\t\t\t\tself.cho[j][k]=delta_weight\n",
    "\n",
    "\t\t# Now for the hidden layer.\n",
    "\t\thidden_deltas = [0.0]*self.nh\n",
    "\t\tfor j in range(self.nh):\n",
    "\t\t\t# Error as given by formule is equal to the sum of (Weight from each node in hidden layer times output delta of output node)\n",
    "\t\t\t# Hence delta for hidden layer = sum (self.who[j][k]*output_deltas[k])\n",
    "\t\t\terror=0.0\n",
    "\t\t\tfor k in range(self.no):\n",
    "\t\t\t\terror+=self.who[j][k] * output_deltas[k]\n",
    "\t\t\t# now, change in node weight is given by dsigmoid() of activation of each hidden node times the error.\n",
    "\t\t\thidden_deltas[j]= error * dsigmoid(self.ah[j])\n",
    "\n",
    "\t\tfor i in range(self.ni):\n",
    "\t\t\tfor j in range(self.nh):\n",
    "\t\t\t\tdelta_weight = hidden_deltas[j] * self.ai[i]\n",
    "\t\t\t\tself.wih[i][j]+= M*self.cih[i][j] + N*delta_weight\n",
    "\t\t\t\tself.cih[i][j]=delta_weight\n",
    "\n",
    "\t# Main testing function. Used after all the training and Backpropagation is completed.\n",
    "\tdef test(self, patterns):\n",
    "\t\tfor p in patterns:\n",
    "\t\t\tinputs = p[0]\n",
    "\t\t\tprint('For input:', p[0], ' Output -->', self.runNetwork(inputs), '\\tTarget: ', p[1]\n",
    ")\n",
    "\n",
    "\t# So, runNetwork was needed because, for every iteration over a pattern [] array, we need to feed the values.\n",
    "\tdef runNetwork(self, feed):\n",
    "\t\tif(len(feed)!=self.ni-1):\n",
    "\t\t\tprint('Error in number of input values.')\n",
    "\n",
    "\t\t# First activate the ni-1 input nodes.\n",
    "\t\tfor i in range(self.ni-1):\n",
    "\t\t\tself.ai[i]=feed[i]\n",
    "\n",
    "\t\t#\n",
    "\t\t# Calculate the activations of each successive layer's nodes.\n",
    "\t\t#\n",
    "\t\tfor j in range(self.nh):\n",
    "\t\t\tsum=0.0\n",
    "\t\t\tfor i in range(self.ni):\n",
    "\t\t\t\tsum+=self.ai[i]*self.wih[i][j]\n",
    "\t\t\t# self.ah[j] will be the sigmoid of sum. # sigmoid(sum)\n",
    "\t\t\tself.ah[j]=sigmoid(sum)\n",
    "\n",
    "\t\tfor k in range(self.no):\n",
    "\t\t\tsum=0.0\n",
    "\t\t\tfor j in range(self.nh):\n",
    "\t\t\t\tsum+=self.ah[j]*self.wih[j][k]\n",
    "\t\t\t# self.ah[k] will be the sigmoid of sum. # sigmoid(sum)\n",
    "\t\t\tself.ao[k]=sigmoid(sum)\n",
    "\n",
    "\t\treturn self.ao\n",
    "\n",
    "\n",
    "\tdef trainNetwork(self, pattern):\n",
    "\t\tfor i in range(500):\n",
    "\t\t\t# Run the network for every set of input values, get the output values and Backpropagate them.\n",
    "\t\t\tfor p in pattern:\n",
    "\t\t\t\t# Run the network for every tuple in p.\n",
    "\t\t\t\tinputs = p[0]\n",
    "\t\t\t\tout = self.runNetwork(inputs)\n",
    "\t\t\t\texpected = p[1]\n",
    "\t\t\t\tself.backpropagate(inputs,expected,out)\n",
    "\t\tself.test(pattern)\n",
    "\n",
    "# End of class.\n",
    "\n",
    "\n",
    "def randomizeMatrix ( matrix, a, b):\n",
    "\tfor i in range ( len (matrix) ):\n",
    "\t\tfor j in range ( len (matrix[0]) ):\n",
    "\t\t\t# For each of the weight matrix elements, assign a random weight uniformly between the two bounds.\n",
    "\t\t\tmatrix[i][j] = random.uniform(a,b)\n",
    "\n",
    "\n",
    "# Now for our function definition. Sigmoid.\n",
    "def sigmoid(x):\n",
    "\treturn 1 / (1 + math.exp(-x))\n",
    "\n",
    "\n",
    "# Sigmoid function derivative.\n",
    "def dsigmoid(y):\n",
    "\treturn y * (1 - y)\n",
    "\n",
    "\n",
    "def main():\n",
    "\t# take the input pattern as a map. Suppose we are working for AND gate.\n",
    "\tpat = [\n",
    "\t\t[[0,0], [0]],\n",
    "\t\t[[0,1], [1]],\n",
    "\t\t[[1,0], [1]],\n",
    "\t\t[[1,1], [1]]\n",
    "\t]\n",
    "\tnewNeural = Neural(pat)\n",
    "\tnewNeural.trainNetwork(pat)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tmain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
